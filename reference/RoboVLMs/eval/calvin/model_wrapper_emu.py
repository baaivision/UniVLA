import json
import os.path
from copy import deepcopy
import torch
from PIL import Image
from typing import Literal
import numpy as np
import functools

from lightning.pytorch.trainer import Trainer

from eval_utils import init_trainer_config, euler2rotm, rotm2euler
from robovlms.train.base_trainer import BaseTrainer
from robovlms.utils.model_utils import build_tokenizer
from robovlms.data.datamodule.gr_datamodule import GRDataModule
from robovlms.data.data_utils import get_text_function
from robovlms.data.data_utils import (
    preprocess_image,
    get_prompt_builder,
    tcp_to_world_frame,
)
from queue import Queue
from robovlms.model.policy_head.action_tokenizer import ActionTokenizer
from robovlms.data.data_utils import unnoramalize_action

# emu3
from transformers import AutoModel, AutoImageProcessor, GenerationConfig, AutoProcessor
from transformers.feature_extraction_utils import BatchFeature
from transformers.generation import LogitsProcessorList, PrefixConstrainedLogitsProcessor, UnbatchedClassifierFreeGuidanceLogitsProcessor
import sys
sys.path.append("/share/project/yuqi.wang/UniVLA/reference/Emu3")
from emu3.mllm import Emu3Tokenizer, Emu3ForCausalLM, Emu3Processor
from emu3.mllm import Emu3MoE
from transformers import LogitsProcessor

class ActionIDConstraintLogitsProcessor(LogitsProcessor):
    def __init__(self, allowed_token_ids):
        """
        :param allowed_token_ids: 允许的token ID列表
        """
        self.allowed_token_ids = allowed_token_ids

    def __call__(self, input_ids, scores):
        # 创建掩码：允许的token位置为True，其他为False
        mask = torch.zeros_like(scores, dtype=torch.bool)
        if mask.ndim == 1:
            mask[self.allowed_token_ids] = True
        else:
            mask[:, self.allowed_token_ids] = True
        
        # 将不允许的token概率设为负无穷
        scores[~mask] = -float("inf")
        return scores

class EmuVLAModel:
    def __init__(self,emu_hub,vq_hub,vision_hub,device,raw_calvin=True):
        self.emu_hub = emu_hub
        self.vq_hub = vq_hub
        self.vision_hub = vision_hub
        self.device = device
        self.raw_calvin = raw_calvin

        # vllm
        self.vllm_accelerator = False
        ## hard code here
        self.window_size = 2
        self.predict_action_frames = 10
        self.normalize_action = True
        self.video_mode = True
        # load model and tokenizer
        self.init_config(device=device)
        self.action_space = "continuous"
        self.image_processor.min_pixels = 80 * 80

        self.predict_frames = 1
        self.context_frames = 1
        self.action_dim = 7
        self.diffusion_steps = 30
        self.classifier_free_guidance = 1.0
        self.target_length = 800

        # flow matching
        # self.use_gripper = True
        # self.use_fast = False
        # self.use_one_step = True

        self.use_gripper = True
        self.use_fast = True
        self.use_one_step = False
        self.eoa_token_id = 151845

        self.kwargs = dict(
            mode='VLA',
            padding="longest",
        )
        if self.use_fast:
            if self.vllm_accelerator:
                pass
            else:
                self.GENERATION_CONFIG = GenerationConfig(
                    pad_token_id=self.model.config.pad_token_id,
                    bos_token_id=self.model.config.bos_token_id,
                    eos_token_id=self.eoa_token_id,
                    do_sample=False,
                )
        
        else:
            self.GENERATION_CONFIG = GenerationConfig(
                use_cache=True,
                eos_token_id=self.model.config.eos_token_id,
                pad_token_id=self.model.config.pad_token_id,
                max_new_tokens=800, # hard code here
                do_sample=True,
                top_k=2048,
                temperature=0.8,
            )


    def init_config(self, device):
        
        if self.vllm_accelerator:
            from vllm import LLM, SamplingParams, TokensPrompt
            self.tokenizer = Emu3Tokenizer.from_pretrained(
                self.vq_hub,
                model_max_length=2000,
                padding_side="right",
                use_fast=False,
            )
            self.model = LLM(
                model=self.emu_hub,
                skip_tokenizer_init=True,
                trust_remote_code=True,
                dtype='bfloat16',
                tensor_parallel_size=1,
                pipeline_parallel_size=1,
                disable_log_stats=False,
                enable_chunked_prefill=True,
                max_seq_len_to_capture=2000,
                gpu_memory_utilization=0.8,
            )
        else:
            self.model = Emu3MoE.from_pretrained(
                self.emu_hub,
                torch_dtype=torch.bfloat16,
                attn_implementation="flash_attention_2",
                trust_remote_code=True,
            )
            self.model.to(device).eval()

            self.tokenizer = Emu3Tokenizer.from_pretrained(
                self.vq_hub,
                model_max_length=self.model.config.max_position_embeddings,
                padding_side="right",
                use_fast=False,
            )
        self.image_processor = AutoImageProcessor.from_pretrained(self.vision_hub, trust_remote_code=True)
        self.image_tokenizer = AutoModel.from_pretrained(self.vision_hub, trust_remote_code=True).to(device).eval()
        self.processor = Emu3Processor(self.image_processor, self.image_tokenizer, self.tokenizer)

        # fast tokenization
        if self.normalize_action:
            fast_path = '/share/project/yuqi.wang/UniVLA/pretrain/fast_calvin_norm_a10_s50'
        else:
            fast_path = "/share/project/yuqi.wang/UniVLA/pretrain/fast"

        self.action_tokenizer = AutoProcessor.from_pretrained(fast_path, trust_remote_code=True)

        self.rgb_list = []
        self.hand_rgb_list = []
        self.action_hist_list = []
        self.rollout_step_counter = 0

        self.vision_queue = Queue(maxsize=self.window_size)
        self.vision_gripper_queue = Queue(maxsize=self.window_size)
        self.action_queue = Queue(maxsize=self.window_size - 1)

    def add_image(self, image):
        if self.vision_queue.full():
            self.vision_queue.get()
        self.vision_queue.put(image)
    
    def get_history(self):
        return list(self.vision_queue.queue) 

    def add_action(self, action):
        if self.action_queue.full():
            self.action_queue.get()
        self.action_queue.put(action)
    
    def get_action_history(self):
        return list(self.action_queue.queue)

    def step(self, obs, goal):
        """Step function."""
        # preprocess observation
        image_code, gripper_code = self.preprocess(obs, goal, self.action_space)

        input_dict = dict()
        prompt,neg_prompt = goal, ""

        video_code = image_code.unsqueeze(1)
        gripper_code = gripper_code.unsqueeze(1) if self.use_gripper else None

        text_prompt = [self.tokenizer.bos_token + prompt]
        text_tokens = self.processor.tokenizer(text_prompt)
        text_tokens = BatchFeature(data={**text_tokens}, tensor_type='pt')

        if self.video_mode:
            kwargs = dict(
                    mode='VLA_Video',
                    padding="longest",
                )
            pos_inputs = self.processor.video_process(text=prompt, video_tokens=video_code, gripper_tokens=gripper_code ,context_frames=self.context_frames, frames = self.predict_frames, return_tensors="pt", **kwargs)
        else:
            pos_inputs = self.processor.video_process(text=prompt, video_tokens=video_code, gripper_tokens=gripper_code ,context_frames=self.context_frames, frames = self.predict_frames, return_tensors="pt", **self.kwargs)
            neg_inputs = self.processor.video_process(text=neg_prompt, video_tokens=video_code, gripper_tokens=gripper_code, context_frames=self.context_frames, frames = self.predict_frames, return_tensors="pt",**self.kwargs)
        
        if self.video_mode:
            self.add_image(pos_inputs)
            
            # 获取历史图像和动作
            history = self.get_history()
            action_history = self.get_action_history()

            # 初始化输入ID、token类型ID和attention mask
            all_input_ids = []
            all_token_type_ids = []
            all_attention_mask = []

            # Add text
            all_input_ids.append(text_tokens['input_ids'])
            all_token_type_ids.append(text_tokens['token_type_ids'])
            all_attention_mask.append(text_tokens['attention_mask'])

            # 遍历历史图像
            for i in range(len(history)):
                img_input_ids = history[i]['input_ids']
                img_token_type_ids = history[i]['token_type_ids']
                img_attention_mask = history[i]['attention_mask']
                
                # 对应的动作
                if i < len(action_history):
                    act_input_ids = action_history[i]
                    
                    # 动作的token_type_ids和attention_mask分别填充为全0和全1
                    act_token_type_ids = torch.zeros_like(act_input_ids)
                    act_attention_mask = torch.ones_like(act_input_ids)
                    
                    # 交替添加图像和动作数据
                    all_input_ids.extend([img_input_ids, act_input_ids])
                    all_token_type_ids.extend([img_token_type_ids, act_token_type_ids])
                    all_attention_mask.extend([img_attention_mask, act_attention_mask])
                else:
                    # 若没有对应的动作，添加图像数据
                    all_input_ids.append(img_input_ids)
                    all_token_type_ids.append(img_token_type_ids)
                    all_attention_mask.append(img_attention_mask)
            # 拼接所有的input_ids、token_type_ids和attention_mask
            concatenated_input_ids = torch.cat(all_input_ids, dim=1)
            concatenated_token_type_ids = torch.cat(all_token_type_ids, dim=1)
            concatenated_attention_mask = torch.cat(all_attention_mask, dim=1)
            
            # 更新pos_inputs
            final_inputs = pos_inputs.copy()
            final_inputs['input_ids'] = concatenated_input_ids
            final_inputs['token_type_ids'] = concatenated_token_type_ids
            final_inputs['attention_mask'] = concatenated_attention_mask
        else:
            final_inputs = pos_inputs

        if self.use_fast: 
            last_token_id = self.tokenizer.pad_token_id - 1
            allowed_token_ids = list(range(last_token_id - self.action_tokenizer.vocab_size, last_token_id + 1)) + [self.eoa_token_id]
            action_id_processor = ActionIDConstraintLogitsProcessor(allowed_token_ids)
            
            with torch.no_grad():
                if self.vllm_accelerator:
                    from vllm import LLM, SamplingParams, TokensPrompt
                    self.GENERATION_CONFIG = SamplingParams(
                        temperature=0.0,            # Set to 0 for deterministic behavior (do_sample=False equivalent)
                        top_p=1.0,                  # No probability filtering
                        top_k=-1,                   # No token count limiting
                        max_tokens=50,             # Maximum tokens to generate
                        stop_token_ids=[self.eoa_token_id],  # End generation at this token
                        skip_special_tokens=False,  # Keep special tokens
                        logits_processors=[action_id_processor],
                    )
                    input_ids_list = final_inputs.input_ids.detach().cpu().tolist()[0]
                    inputs = TokensPrompt(prompt_token_ids=input_ids_list)
                    outputs = self.model.generate(inputs,self.GENERATION_CONFIG)
                    token_ids_tuple = outputs[0].outputs[0].token_ids
                    token_ids_list = list(token_ids_tuple)
                    outputs = torch.tensor([token_ids_list], dtype=torch.long)
                else:
                    outputs = self.model.generate(
                        final_inputs.input_ids.to(self.device),
                        self.GENERATION_CONFIG,
                        max_new_tokens=100,
                        logits_processor=[action_id_processor],
                        attention_mask=final_inputs.attention_mask.to(self.device),
                    )
            # omit the eoa token
            if self.vllm_accelerator:
                orig_outputs = outputs
                outputs = outputs[:, :-1]
            else:
                orig_outputs = outputs[:, final_inputs.input_ids.shape[-1]:]
                outputs = outputs[:, final_inputs.input_ids.shape[-1]:-1]
            last_token_id_tensor = torch.tensor(last_token_id, dtype=outputs.dtype, device=outputs.device)
            processed_outputs = last_token_id_tensor - outputs
            action_outputs = self.action_tokenizer.decode(
                processed_outputs, time_horizon=self.predict_action_frames, action_dim=self.action_dim
            )
            action = torch.from_numpy(action_outputs[0])
            if self.video_mode:
                self.add_action(orig_outputs.detach().cpu())

        else:
            # logits_processor
            h = pos_inputs.video_size[:, 0]
            w = pos_inputs.video_size[:, 1]
            t = pos_inputs.video_size[:, 2]
            constrained_fn = self.processor.build_prefix_constrained_fn_video(h, w, t)
            logits_processor = LogitsProcessorList([
                UnbatchedClassifierFreeGuidanceLogitsProcessor(
                    self.classifier_free_guidance,
                    self.model,
                    unconditional_ids=neg_inputs.input_ids.to(self.device),
                ),
                PrefixConstrainedLogitsProcessor(
                    constrained_fn,
                    num_beams=1,
                ),
            ])
            # model inference
            with torch.no_grad():
                worldmodel_outputs = self.model.generate(
                            pos_inputs.input_ids.to(self.device),
                            self.GENERATION_CONFIG,
                            logits_processor=logits_processor,
                            attention_mask=pos_inputs.attention_mask.to(self.device),
                        )   
                # worldmodel_outputs = pos_inputs.input_ids.to(self.device)
                padding = torch.full((1, self.target_length - worldmodel_outputs.shape[1]), self.model.config.pad_token_id, dtype=worldmodel_outputs.dtype).to(worldmodel_outputs.device)
                worldmodel_outputs = torch.cat([worldmodel_outputs, padding], dim=1)        
                action_outputs = self.model.generate_action(
                        outputs = worldmodel_outputs,
                        sample_steps = self.diffusion_steps,
                        frames = self.predict_action_frames,
                        action_dim = self.action_dim,
                    )    
            action = action_outputs[0].detach().cpu()

        self.rollout_step_counter += 1

        if self.normalize_action:
            action = self.unnormalize_action(action)

        if action.ndim >= 2:
            action[..., -1] = torch.where(action[..., -1] > 0, 1, -1)
        else:
            action[-1] = 1 if action[-1] > 0 else -1

        if self.use_one_step:
            # only one step
            action_pred = np.array(action[0].to(torch.float32))
        else:
            # action chunk
            action_pred = np.array(action.to(torch.float32))
        # print(f"step {self.rollout_step_counter} action {action_pred}")
        return action_pred

    def unnormalize_action(self, action):
        # partial
        # action_high = torch.tensor([
        #     0.68240000006824,
        #     0.5500000000549998,
        #     0.5940000000593999,
        #     0.4292000000429199,
        #     0.40320000004032,
        #     0.9996000000999599,
        #     0.9996000000999599
        # ], dtype=action.dtype, device=action.device)
        # action_low = torch.tensor([
        #     -0.70240000007024,
        #     -0.56760000005676,
        #     -0.430000000043,
        #     -0.42280000004228,
        #     -0.45240000004524006,
        #     -1.0000000001,
        #     -1.0000000001
        # ], dtype=action.dtype, device=action.device)

        # ABC
        # action_high = torch.tensor([
        #     0.68480000006848,
        #     0.5612000000561199,
        #     0.5952000000595199,
        #     0.4340000000433999,
        #     0.42280000004228,
        #     0.9996000000999599,
        #     0.9996000000999599
        # ], dtype=action.dtype, device=action.device)
        
        # action_low = torch.tensor([
        #     -0.7080000000708,
        #     -0.57840000005784,
        #     -0.43320000004332004,
        #     -0.42760000004276,
        #     -0.47640000004764005,
        #     -1.0000000001,
        #     -1.0000000001
        # ], dtype=action.dtype, device=action.device)

        # ABCD
        action_high = torch.tensor([
            0.67640000006764,
            0.5560000000555998,
            0.5944000000594398,
            0.42640000004264,
            0.41200000004119985,
            0.9996000000999599,
            0.9996000000999599
        ], dtype=action.dtype, device=action.device)
        
        action_low = torch.tensor([
            -0.69960000006996,
            -0.57760000005776,
            -0.4336000000433601,
            -0.42320000004232006,
            -0.46520000004652007,
            -1.0000000001,
            -1.0000000001
        ], dtype=action.dtype, device=action.device)
        
        action = 0.5 * (action + 1) * (action_high - action_low) + action_low
        return action

    def preprocess(self, obs, lang, mode="continuous"):
        # preprocess image
        image = obs["rgb_obs"]["rgb_static"]
        image = Image.fromarray(image)
        image_x = self.image_processor(image, return_tensors="pt")["pixel_values"].cuda()
        image_code = self.image_tokenizer.encode(image_x)
        
        gripper_x = None
        if "rgb_gripper" in obs["rgb_obs"]:
            gripper = obs["rgb_obs"]["rgb_gripper"]
            gripper = Image.fromarray(gripper)
            gripper = gripper.resize((80, 80))
            gripper_x = self.image_processor(gripper, return_tensors="pt")["pixel_values"].cuda()
            gripper_code = self.image_tokenizer.encode(gripper_x)

        return (
            image_code,
            gripper_code,
        )

    def reset(self):

        self.rgb_list = []
        self.hand_rgb_list = []
        self.rollout_step_counter = 0
        self.action_hist_list = []

        while not self.vision_queue.empty():
            self.vision_queue.get()
        while not self.vision_gripper_queue.empty():
            self.vision_gripper_queue.get()
        while not self.action_queue.empty():
            self.action_queue.get()



class CustomModel:
    # model option
    def __init__(
        self,
        ckpt_path,
        configs,
        device,
        save_dir=None,
        raw_calvin=True,
        debug=False,
        action_ensemble=False,
    ):
        self.model = BaseTrainer(configs=configs)
        self.init_config(ckpt_path, configs, device, save_dir, raw_calvin, debug)
        # self.model.model.lm_head.window_size = 1

    def init_config(
        self, ckpt_path, configs, device, save_dir=None, raw_calvin=False, debug=False
    ):
        ### load and convert checkpoint
        self.debug = debug
        if configs["model"] == "kosmos":
            import transformers

            import pdb 
            pdb.set_trace()
            package_dir = transformers.__path__[0]
            os.system(
                "cp tools/modeling_kosmos2.py {}/models/kosmos2/modeling_kosmos2.py".format(
                    package_dir
                )
            )

        if not self.debug:
            ckpt = torch.load(ckpt_path, map_location="cpu")
            if "state_dict" in ckpt:
                new_state_dict = ckpt["state_dict"]
            elif "model_state_dict" in ckpt:
                new_state_dict = ckpt["model_state_dict"]
            else:
                raise KeyError("no checkpoint dict in the loaded pretrain parameters")

            new_state_dict = self.convert_old_state_dict(new_state_dict)
            msg = self.model.load_state_dict(new_state_dict, strict=False)
            print(f"CKPT Loaded \n {msg}")

            ckpt_dir = os.path.dirname(ckpt_path)
            ckpt_name = os.path.basename(ckpt_path)
            save_dir = ckpt_dir if save_dir is None else save_dir
            load_info_path = os.path.join(save_dir, f"{ckpt_name}_loading_msg.json")
            if os.path.exists(load_info_path):
                os.system(f"rm {load_info_path}")
            with open(load_info_path, "w") as f:
                _info = {
                    "missing_keys": msg.missing_keys,
                    "unexpected_keys": msg.unexpected_keys,
                }
                json.dump(_info, f, indent=2)
                print(f"Model loading msg is updated to: {load_info_path}")

        self.configs = configs

        dtype = torch.float32
        if self.configs["trainer"]["precision"] == "bf16":
            dtype = torch.bfloat16
        elif self.configs["trainer"]["precision"] == "fp16":
            dtype = torch.float16
        self.dtype = dtype
        self.act_head_configs = self.configs["act_head"]
        self.raw_calvin = raw_calvin
        self.tcp_rel = self.configs.get("tcp_rel", False)

        print(f"raw action: {self.raw_calvin}")

        self.device = device
        self.policy = self.model
        self.policy = self.policy.to(self.dtype)
        # self.policy = self.policy.float()
        self.policy.to(self.device)
        self.policy.eval()

        if not hasattr(self.policy.model, "lm_head"):
            self.policy.model.lm_head = self.policy.model.act_head

        self.tokenizer = build_tokenizer(self.configs["tokenizer"])

        self.window_size = configs["window_size"]
        self.fwd_pred_next_n = configs["fwd_pred_next_n"]
        self.act_step = self.fwd_pred_next_n + 1
        self.seq_len = self.configs["seq_len"]
        self.use_hand_rgb = self.configs["use_hand_rgb"]

        if hasattr(self, "policy_setup"):
            data_mix = "bridge" if self.policy_setup == "widowx_bridge" else "rt_1"
            configs["train_dataset"]["data_mix"] = data_mix
            configs["val_dataset"]["data_mix"] = data_mix
        if configs["model"] == "kosmos":
            image_preprocess = self.model.model.image_processor
        else:
            image_preprocess = self.model.model.processor
        self.image_preprocess = functools.partial(
            preprocess_image,
            image_processor=image_preprocess,
            model_type=configs["model"],
        )

        self.text_preprocess = get_text_function(
            self.model.model.tokenizer, configs["model"]
        )

        self.action_space = self.configs["act_head"].get("action_space", "continuous")
        if self.action_space == "discrete":
            self.action_tokenizer = ActionTokenizer(
                self.tokenizer,
                bins=self.act_head_configs["n_bin"],
                min_action=self.act_head_configs["min_action"],
                max_action=self.act_head_configs["max_action"],
            )

        print(f"Evaluating checkpoint {ckpt_path}")

        self.rgb_list = []
        self.hand_rgb_list = []
        self.action_hist_list = []
        self.rollout_step_counter = 0

        self.vision_queue = Queue(maxsize=self.window_size)
        self.vision_gripper_queue = Queue(maxsize=self.window_size)
        self.action_queue = Queue(maxsize=self.window_size - 1)

    def ensemble_action(self, action):
        if action.ndim >= 3:
            action = action.squeeze()

        if action.ndim == 1:
            action = action.unsqueeze(0)

        self.action_hist_list.append(action)

        act_cache = []
        # self.fwd_pred_next_n = 1
        max_len = self.fwd_pred_next_n
        # max_len = 1
        # if self.tcp_rel:
        #     max_len = 1
        while len(self.action_hist_list) > max_len:
            self.action_hist_list.pop(0)

        idx = 0
        for act in self.action_hist_list[::-1]:
            # print(act.shape)
            act_cache.append(act[idx])
            idx += 1

        act_cache = torch.stack(act_cache, dim=0)

        weights = torch.tensor([fwd_decay_ratio**i for i in range(len(act_cache))])
        weights = weights / weights.sum()

        weighted_act = (act_cache * weights.unsqueeze(1)).sum(dim=0)

        return weighted_act

    @staticmethod
    def convert_old_state_dict(state_dict):
        new_state_dict = {}
        for k, v in state_dict.items():
            if k.startswith("module."):
                new_k = k.replace("module.", "")
            else:
                new_k = k

            if not new_k.startswith("model."):
                new_k = "model." + new_k

            new_state_dict[new_k] = state_dict[k]
        return new_state_dict

    def _get_default_calvin_config(self):
        return {
            "type": "DiskCalvinDataset",
            "data_dir": "CALVIN/task_ABCD_D/val",
            "c_act_scaler": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0],
        }

    def add_element_to_queue(self, q: Queue, element):
        while q.qsize() >= q.maxsize:
            q.get()
        q.put(element)

    def get_history(self, q: Queue, pad: Literal["zero", "first"] = "zero"):
        queue_list = list(q.queue)
        if len(queue_list) == 0:
            return queue_list, None
        history_type = self.configs["act_head"].get("history_type", "pre")
        if history_type == "pre":
            pad_len = 0
        else:
            raise ValueError(f"Unsupported history type {history_type}")
        element = queue_list[0]
        if pad == "zero":
            if isinstance(element, torch.Tensor):
                element = torch.zeros_like(element)
            elif isinstance(element, np.ndarray):
                element = np.zeros_like(element)
            else:
                raise ValueError("This type is not supported")
            queue_list = [element for _ in range(pad_len)] + queue_list
        else:
            if isinstance(element, torch.Tensor):
                pad_list = [element.clone() for _ in range(pad_len)]
            elif isinstance(element, np.ndarray):
                pad_list = [deepcopy(element) for _ in range(pad_len)]
            queue_list = pad_list + queue_list
        pad_mask = np.ones(q.maxsize, dtype=bool)
        pad_mask[:pad_len] = False
        return queue_list, pad_mask

    def preprocess(self, obs, lang, mode="continuous"):
        # preprocess image
        image = obs["rgb_obs"]["rgb_static"]
        image = Image.fromarray(image)
        image_x = self.image_preprocess([image]).unsqueeze(0)

        gripper_x = None
        if "rgb_gripper" in obs["rgb_obs"]:
            gripper = obs["rgb_obs"]["rgb_gripper"]
            gripper = Image.fromarray(gripper)
            gripper_x = self.image_preprocess([gripper]).unsqueeze(0)
            gripper_x = gripper_x.to(self.device).to(self.dtype)

        if self.configs["act_head"].get("history_type", "post") == "pre":
            self.add_element_to_queue(self.vision_queue, image_x)
            image_x, _ = self.get_history(self.vision_queue, pad="first")
            image_x = torch.concatenate(image_x, dim=1)

            if gripper_x is not None:
                self.add_element_to_queue(self.vision_gripper_queue, gripper_x)
                gripper_x, _ = self.get_history(self.vision_gripper_queue, pad="first")
                gripper_x = (
                    torch.concatenate(gripper_x, dim=1).to(self.device).to(self.dtype)
                )

        if mode == "discrete":
            if "llava" in self.policy.configs:
                model_name = self.policy.configs["llava"]
            elif "qwen" in self.policy.configs:
                model_name = "qwen"
            else:
                # model_name = self.policy.configs['llm']['pretrained_model_name_or_path']
                model_name = self.policy.configs["model"]

            prompt_builder = get_prompt_builder(
                model_name, bos=self.tokenizer.bos_token, eos=self.tokenizer.eos_token
            )

            conversation = [
                {
                    "from": "human",
                    "value": (
                        f"What action should the robot take to {lang}?"
                        if self.act_step == 1
                        else f"What {self.act_step} step actions should the robot take to {lang}?"
                    ),
                },
                {"from": "gpt", "value": ""},
            ]

            input_ids = []
            for turn in conversation:
                prompt_builder.add_turn(turn["from"], turn["value"])

            input_ids = torch.tensor(
                list(
                    self.tokenizer(
                        prompt_builder.get_prompt(), add_special_tokens=True
                    ).input_ids
                )
            )
            if self.tokenizer.eos_token is not None:
                input_ids = input_ids[:-1]

            text_x = input_ids.unsqueeze(0)
            mask = torch.full((1, text_x.shape[-1]), True, dtype=torch.bool)
        else:
            text_x, mask = self.text_preprocess([lang])

        return (
            image_x.to(self.device).to(self.dtype),
            gripper_x,
            text_x.to(self.device),
            mask.to(self.device),
        )

    def step(self, obs, goal):
        """Step function."""
        input_dict = dict()
        image_x, gripper_x, text_x, mask = self.preprocess(obs, goal, self.action_space)

        input_dict["rgb"] = image_x
        input_dict["hand_rgb"] = gripper_x
        input_dict["text"] = text_x
        input_dict["text_mask"] = mask

        if self.action_space == "discrete":
            input_dict["instr_and_action_ids"] = text_x
            input_dict["instr_and_action_mask"] = mask

        with torch.no_grad():
            action = self.policy.inference_step(input_dict)["action"]
        if self.action_space != "discrete":
            # print(action)
            if action[0].ndim == action[1].ndim + 1:
                action = (action[0], action[1].unsqueeze(2))
            action = torch.cat(
                [action[0], (torch.nn.functional.sigmoid(action[1]) > 0.5).float()],
                dim=-1,
            )

        # action = action[0, 0, 0] # batch, seq_len, chunck_idx
        if isinstance(action, tuple):
            action = torch.cat([action[0], action[1]], dim=-1)

        if isinstance(action, np.ndarray):
            action = torch.from_numpy(action)

        if action.ndim == 2:
            action = action.unsqueeze(1)

        if action.ndim == 3:
            action = action.unsqueeze(1)

        action = action.detach().cpu()

        if self.tcp_rel:
            robot_obs = (
                torch.from_numpy(obs["robot_obs"])
                .unsqueeze(0)
                .unsqueeze(0)
                .unsqueeze(0)
                .repeat(1, 1, self.fwd_pred_next_n, 1)
            )
            action = tcp_to_world_frame(action, robot_obs)

        # action = self.ensemble_action(action)
        if action.ndim == 4:
            action = action.squeeze().squeeze()

        # if isinstance(action, torch.Tensor):
        #     action = action.squeeze()
        #     if action.ndim == 2:
        #         action = action[0]
            # action = action.numpy()
        if self.configs.get("use_mu_law", False):
            from robovlms.data.data_utils import inverse_mu_law_companding

            action = inverse_mu_law_companding(
                action, self.configs.get("mu_val", 255), maintain_last=True
            )
        if self.configs.get("norm_action", False):
            from robovlms.data.data_utils import unnoramalize_action

            if isinstance(action, tuple):
                action = (
                    unnoramalize_action(
                        action[0], self.configs["norm_min"], self.configs["norm_max"]
                    ),
                    action[1],
                )
            else:
                action = unnoramalize_action(
                    action, self.configs["norm_min"], self.configs["norm_max"]
                )

        if self.action_space == "discrete":
            # action[-1] = 1 if action[-1] > 0 else -1
            pass
        else:
            if self.raw_calvin:
                if action.ndim == 2:
                    action[:,-1] = (action[:,-1] - 0.5) * 2
                else:
                    action[-1] = (action[-1] - 0.5) * 2
            else:
                state = obs["robot_obs"]  # (15,)
                xyz_state = state[:3]
                rpy_state = state[3:6]
                rotm_state = euler2rotm(rpy_state)
                rel_action = action.numpy()
                _c_rel_action = rel_action[:6]
                xyz_action = _c_rel_action[:3]
                rpy_action = _c_rel_action[3:6]
                gripper_action = rel_action[6]
                rotm_action = euler2rotm(rpy_action)
                xyz_next_state = xyz_state + rotm_state @ xyz_action
                rotm_next_state = rotm_state @ rotm_action
                rpy_next_state = rotm2euler(rotm_next_state)

                action = action.numpy()
                action[:3] = xyz_next_state - xyz_state
                action[3:6] = rpy_next_state - rpy_state
                action[:6] *= [50.0, 50.0, 50.0, 20.0, 20.0, 20.0]
                action[-1] = (gripper_action - 0.5) * 2
                action = torch.from_numpy(action)

        self.rollout_step_counter += 1
        if action.ndim >= 2:
            action[..., -1] = torch.where(action[..., -1] > 0, 1, -1)
        else:
            action[-1] = 1 if action[-1] > 0 else -1
        # print(f"step {self.rollout_step_counter} action {action}")
        return np.array(action)

    def reset(self):
        if hasattr(self.model.model, "lm_head"):
            self.model.model.lm_head.hidden_state = None
            self.model.model.lm_head.history_memory = []
        if hasattr(self.model.model, "act_head"):
            self.model.model.act_head.hidden_state = None
            self.model.model.act_head.history_memory = []

        self.rgb_list = []
        self.hand_rgb_list = []
        self.rollout_step_counter = 0
        self.action_hist_list = []

        while not self.vision_queue.empty():
            self.vision_queue.get()
        while not self.vision_gripper_queue.empty():
            self.vision_gripper_queue.get()
        while not self.action_queue.empty():
            self.action_queue.get()
